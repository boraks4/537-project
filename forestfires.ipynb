{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "forestfires.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boraks4/539-project/blob/main/forestfires.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Credits to https://stackoverflow.com/a/57539179\n",
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "\n",
        "user = input('User name: ')\n",
        "password = getpass('Password: ')\n",
        "password = urllib.parse.quote(password) # your password is converted into url format\n",
        "\n",
        "cmd_string = 'git clone https://{0}:{1}@github.com/boraks4/539-project.git'.format(user, password)\n",
        "\n",
        "os.system(cmd_string)\n",
        "cmd_string, password = \"\", \"\" # removing the password from the variable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "merjZ6yJcTtX",
        "outputId": "971f55ee-2bbc-46b1-f775-9d2870f4d102"
      },
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User name: boraks4\n",
            "Password: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd 539-project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dMW3uMDa4bW",
        "outputId": "b81362b5-528d-46fc-e80b-2be0903ebf0d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/539-project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import resample\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "svgckzvrYDhP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fires = pd.read_csv('forestfires.csv', sep=',', header=0)\n",
        "# TODO: Is this the encoding we want for months, days?\n",
        "fires.month=fires.month.map({'jan':1,'feb':2,'mar':3,'apr':4,'may':5,'jun':6,'jul':7,'aug':8,'sep':9,'oct':10,'nov':11,'dec':12})\n",
        "fires.day=fires.day.map({'mon':1,'tue':2,'wed':3,'thu':4,'fri':5,'sat':6,'sun':7}) \n",
        "\n",
        "print(fires)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Iaed6PPYT_R",
        "outputId": "5c487182-8dc7-4f0a-b1ae-ecf0e4f8d54e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     X  Y  month  day  FFMC    DMC     DC   ISI  temp  RH  wind  rain   area\n",
            "0    7  5      3    5  86.2   26.2   94.3   5.1   8.2  51   6.7   0.0   0.00\n",
            "1    7  4     10    2  90.6   35.4  669.1   6.7  18.0  33   0.9   0.0   0.00\n",
            "2    7  4     10    6  90.6   43.7  686.9   6.7  14.6  33   1.3   0.0   0.00\n",
            "3    8  6      3    5  91.7   33.3   77.5   9.0   8.3  97   4.0   0.2   0.00\n",
            "4    8  6      3    7  89.3   51.3  102.2   9.6  11.4  99   1.8   0.0   0.00\n",
            "..  .. ..    ...  ...   ...    ...    ...   ...   ...  ..   ...   ...    ...\n",
            "512  4  3      8    7  81.6   56.7  665.6   1.9  27.8  32   2.7   0.0   6.44\n",
            "513  2  4      8    7  81.6   56.7  665.6   1.9  21.9  71   5.8   0.0  54.29\n",
            "514  7  4      8    7  81.6   56.7  665.6   1.9  21.2  70   6.7   0.0  11.16\n",
            "515  1  4      8    6  94.4  146.0  614.7  11.3  25.6  42   4.0   0.0   0.00\n",
            "516  6  3     11    2  79.5    3.0  106.7   1.1  11.8  31   4.5   0.0   0.00\n",
            "\n",
            "[517 rows x 13 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zeros = fires[fires['area'] == 0]\n",
        "zeros = zeros.assign(size=0)\n",
        "\n",
        "no_zeros = fires[fires['area'] != 0]\n",
        "no_zeros = no_zeros.assign(size=pd.qcut(no_zeros['area'], 3, labels=[1, 2, 3]))\n",
        "\n",
        "fires_quant = pd.concat([zeros, no_zeros])\n",
        "fires_quant = fires_quant.drop(['area'], axis=1)\n",
        "print(fires_quant)\n",
        "print(fires_quant['size'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWIF62Np7_Jx",
        "outputId": "f80637df-7a03-4256-d48b-6fa197f7137f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     X  Y  month  day  FFMC    DMC     DC  ISI  temp  RH  wind  rain  size\n",
            "0    7  5      3    5  86.2   26.2   94.3  5.1   8.2  51   6.7   0.0     0\n",
            "1    7  4     10    2  90.6   35.4  669.1  6.7  18.0  33   0.9   0.0     0\n",
            "2    7  4     10    6  90.6   43.7  686.9  6.7  14.6  33   1.3   0.0     0\n",
            "3    8  6      3    5  91.7   33.3   77.5  9.0   8.3  97   4.0   0.2     0\n",
            "4    8  6      3    7  89.3   51.3  102.2  9.6  11.4  99   1.8   0.0     0\n",
            "..  .. ..    ...  ...   ...    ...    ...  ...   ...  ..   ...   ...   ...\n",
            "509  5  4      8    5  91.0  166.9  752.6  7.1  21.1  71   7.6   1.4     1\n",
            "510  6  5      8    5  91.0  166.9  752.6  7.1  18.2  62   5.4   0.0     1\n",
            "512  4  3      8    7  81.6   56.7  665.6  1.9  27.8  32   2.7   0.0     2\n",
            "513  2  4      8    7  81.6   56.7  665.6  1.9  21.9  71   5.8   0.0     3\n",
            "514  7  4      8    7  81.6   56.7  665.6  1.9  21.2  70   6.7   0.0     3\n",
            "\n",
            "[517 rows x 13 columns]\n",
            "0    247\n",
            "1     90\n",
            "2     90\n",
            "3     90\n",
            "Name: size, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# label = []\n",
        "# for a in fires['area']:\n",
        "#   if a > 50:\n",
        "#     label.append(3) # 'catastrophic'\n",
        "#   elif a > 10: \n",
        "#     label.append(2) # 'large'\n",
        "#   elif a > 0:\n",
        "#     label.append(1) # 'medium'\n",
        "#   else:\n",
        "#     label.append(0) # 'small'\n",
        "# fires['classification'] = label\n",
        "# fires = fires.drop(['area'], axis=1)\n",
        "# pd.options.display.max_columns = len(fires.columns)\n",
        "# pd.options.display.width = 100\n",
        "# print(fires)\n"
      ],
      "metadata": {
        "id": "kopGlpGvv5kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paste in hw8 solution"
      ],
      "metadata": {
        "id": "S7v1o-A_BazT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7rx1rzqy25o"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = fires_quant.iloc[:,:-1]\n",
        "print(X)\n",
        "y = fires_quant.iloc[:,-1]\n"
      ],
      "metadata": {
        "id": "KuYJ7JkDDMK6",
        "outputId": "4f73faca-3281-4a95-8186-ca86abaff6b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     X  Y  month  day  FFMC    DMC     DC  ISI  temp  RH  wind  rain\n",
            "0    7  5      3    5  86.2   26.2   94.3  5.1   8.2  51   6.7   0.0\n",
            "1    7  4     10    2  90.6   35.4  669.1  6.7  18.0  33   0.9   0.0\n",
            "2    7  4     10    6  90.6   43.7  686.9  6.7  14.6  33   1.3   0.0\n",
            "3    8  6      3    5  91.7   33.3   77.5  9.0   8.3  97   4.0   0.2\n",
            "4    8  6      3    7  89.3   51.3  102.2  9.6  11.4  99   1.8   0.0\n",
            "..  .. ..    ...  ...   ...    ...    ...  ...   ...  ..   ...   ...\n",
            "509  5  4      8    5  91.0  166.9  752.6  7.1  21.1  71   7.6   1.4\n",
            "510  6  5      8    5  91.0  166.9  752.6  7.1  18.2  62   5.4   0.0\n",
            "512  4  3      8    7  81.6   56.7  665.6  1.9  27.8  32   2.7   0.0\n",
            "513  2  4      8    7  81.6   56.7  665.6  1.9  21.9  71   5.8   0.0\n",
            "514  7  4      8    7  81.6   56.7  665.6  1.9  21.2  70   6.7   0.0\n",
            "\n",
            "[517 rows x 12 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "unnecessary, already one-hot starting with 0\n",
        "cats = np.unique(y)\n",
        "# reformmat y label so starts at 0 for one-hot encoding\n",
        "y = y - cats[0]\n",
        "'''\n"
      ],
      "metadata": {
        "id": "W47hS7daDQId",
        "outputId": "43de6622-9d55-49bf-9576-2f0dd1dce669",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nunnecessary, already one-hot starting with 0\\ncats = np.unique(y)\\n# reformmat y label so starts at 0 for one-hot encoding\\ny = y - cats[0]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "# hyperparameters\n",
        "\n",
        "# splitting data\n",
        "splits = (.6,.2,.2) # train, test, validatate at 60/20/20 division\n",
        "rand_state = 0\n",
        "\n",
        "# optimizer\n",
        "lr = 0.001\n",
        "\n",
        "# model creation\n",
        "num_hidden_layers = 1\n",
        "neurons_per_hidden_layer = 4\n",
        "\n",
        "# model trainin\n",
        "num_epochs = 1000\n"
      ],
      "metadata": {
        "id": "nzrcaQTpH4qv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/55119651/downsampling-for-more-than-2-classes\n",
        "def downsample(X, y, label):\n",
        "  data = pd.concat([X, y], axis=1)\n",
        "  g = data.groupby(label, group_keys=False)\n",
        "  balanced = pd.DataFrame(g.apply(lambda x: x.sample(g.size().min()))).reset_index(drop=True)\n",
        "  return balanced.iloc[:, :-1], balanced.iloc[:, -1]\n",
        "\n",
        "def pipeline(X, y, label):\n",
        "  X, y = downsample(X, y, label)\n",
        "  return X, pd.get_dummies(y, prefix=label)\n",
        "\n",
        "# partition into train, validate, test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=splits[0], random_state=rand_state, shuffle=True, stratify=y)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, train_size=(splits[1] / (splits[1] + splits[2])), random_state=rand_state, shuffle=True, stratify=y_temp)\n",
        "\n",
        "label='size'\n",
        "\n",
        "X_train, y_train = pipeline(X_train, y_train, 'size')\n",
        "X_test, y_test = pipeline(X_test, y_test, 'size')\n",
        "X_val, y_val = pipeline(X_val, y_val, 'size')\n",
        "\n",
        "print(X_train)\n",
        "\n",
        "scale_cols = ['FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain']\n",
        "\n",
        "scaler = StandardScaler().fit(X_train[scale_cols])\n",
        "X_train[scale_cols] = scaler.transform(X_train[scale_cols])\n",
        "X_test[scale_cols] = scaler.transform(X_test[scale_cols])\n",
        "X_val[scale_cols] = scaler.transform(X_val[scale_cols])\n",
        "\n",
        "print(X_train)\n",
        "\n",
        "batch_size = math.floor(X_train.shape[0] / 10)\n",
        "print(batch_size)"
      ],
      "metadata": {
        "id": "jlB1FP7kDRWI",
        "outputId": "e7f1b273-a193-4fce-99c4-d2ef0ab93fab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     X  Y  month  day  FFMC    DMC     DC   ISI  temp  RH  wind  rain\n",
            "0    7  4     10    6  90.6   43.7  686.9   6.7  14.6  33   1.3   0.0\n",
            "1    2  4      9    7  50.4   46.2  706.6   0.4  12.2  78   6.3   0.0\n",
            "2    6  3      9    1  91.5  130.1  807.1   7.5  20.6  37   1.8   0.0\n",
            "3    8  6      8    1  92.1  207.0  672.6   8.2  21.1  54   2.2   0.0\n",
            "4    3  5      3    1  87.6   52.2  103.8   5.0   9.0  49   2.2   0.0\n",
            "..  .. ..    ...  ...   ...    ...    ...   ...   ...  ..   ...   ...\n",
            "211  5  4      8    2  95.1  141.3  605.8  17.7  26.4  34   3.6   0.0\n",
            "212  4  4      9    5  92.1   99.0  745.3   9.6  20.8  35   4.9   0.0\n",
            "213  6  5      3    1  90.1   39.7   86.6   6.2  15.2  27   3.1   0.0\n",
            "214  7  4      9    3  90.1   82.9  735.7   6.2  15.4  57   4.5   0.0\n",
            "215  4  5      2    7  85.0    9.0   56.9   3.5  10.1  62   1.8   0.0\n",
            "\n",
            "[216 rows x 12 columns]\n",
            "     X  Y  month  day      FFMC       DMC        DC       ISI      temp  \\\n",
            "0    7  4     10    6 -0.041146 -1.101754  0.530810 -0.562122 -0.716191   \n",
            "1    2  4      9    7 -8.691946 -1.061711  0.611845 -2.133475 -1.111457   \n",
            "2    6  3      9    1  0.152529  0.282151  1.025249 -0.362585  0.271974   \n",
            "3    8  6      8    1  0.281645  1.513891  0.471987 -0.187990  0.354321   \n",
            "4    3  5      3    1 -0.686728 -0.965606 -1.867758 -0.986138 -1.638478   \n",
            "..  .. ..    ...  ...       ...       ...       ...       ...       ...   \n",
            "211  5  4      8    2  0.927227  0.461547  0.197207  2.181512  1.227200   \n",
            "212  4  4      9    5  0.281645 -0.215991  0.771037  0.161200  0.304913   \n",
            "213  6  5      3    1 -0.148743 -1.165824 -1.938509 -0.686832 -0.617374   \n",
            "214  7  4      9    3 -0.148743 -0.473871  0.731547 -0.686832 -0.584435   \n",
            "215  4  5      2    7 -1.246232 -1.657559 -2.060680 -1.360270 -1.457314   \n",
            "\n",
            "           RH      wind      rain  \n",
            "0   -0.687880 -1.545158 -0.076176  \n",
            "1    2.093400  1.282788 -0.076176  \n",
            "2   -0.440655 -1.262364 -0.076176  \n",
            "3    0.610050 -1.036128 -0.076176  \n",
            "4    0.301019 -1.036128 -0.076176  \n",
            "..        ...       ...       ...  \n",
            "211 -0.626074 -0.244303 -0.076176  \n",
            "212 -0.564268  0.490963 -0.076176  \n",
            "213 -1.058718 -0.527098 -0.076176  \n",
            "214  0.795469  0.264727 -0.076176  \n",
            "215  1.104500 -1.262364 -0.076176  \n",
            "\n",
            "[216 rows x 12 columns]\n",
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)\n"
      ],
      "metadata": {
        "id": "47OG36GcDSHu",
        "outputId": "964531d9-d702-440d-efa9-d20f5fd73e64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(216, 12)\n",
            "(72, 12)\n",
            "(72, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Preprocessing - normaization?"
      ],
      "metadata": {
        "id": "00mPJfUPRYUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = X.shape[1]\n",
        "num_classes = 4\n",
        "# define the keras model\n",
        "# N_input - neurons_per_hidden_layer - N_labels configuration, relu and sigmoid activation for the \n",
        "# hidden layer and output layer respectively\n",
        "\n",
        "#TODO: dynamic number of layers :)\n",
        "net = tf.keras.models.Sequential()\n",
        "net.add(tf.keras.layers.Dense(units=neurons_per_hidden_layer, input_dim=input_dim, activation = 'relu')) # input layer\n",
        "for l in range(num_hidden_layers):\n",
        "  net.add(tf.keras.layers.Dense(units=neurons_per_hidden_layer, activation = 'relu')) # deep layer\n",
        "net.add(tf.keras.layers.Dense(units=num_classes, activation='softmax')) # output layer"
      ],
      "metadata": {
        "id": "Cc9wmG7KDTox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the keras model\n",
        "opt = tf.keras.optimizers.Adam(\n",
        "    learning_rate=lr\n",
        ")\n",
        "\n",
        "net.compile(loss='CategoricalCrossentropy', optimizer=opt, \n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "QPzedSr2DUmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the keras model on the dataset\n",
        "history = net.fit(X_train, y_train, epochs=num_epochs, verbose=1, batch_size=batch_size, validation_data=(X_val,y_val))"
      ],
      "metadata": {
        "id": "UM01KzR6QF_3",
        "outputId": "2c5150d5-80ef-49f2-c427-c8d1a478921a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "11/11 [==============================] - 1s 24ms/step - loss: 1.8059 - accuracy: 0.2269 - val_loss: 1.6598 - val_accuracy: 0.2500\n",
            "Epoch 2/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.6909 - accuracy: 0.2269 - val_loss: 1.5691 - val_accuracy: 0.2500\n",
            "Epoch 3/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.6030 - accuracy: 0.2176 - val_loss: 1.5041 - val_accuracy: 0.2500\n",
            "Epoch 4/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.5398 - accuracy: 0.2269 - val_loss: 1.4581 - val_accuracy: 0.2500\n",
            "Epoch 5/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.4918 - accuracy: 0.2269 - val_loss: 1.4296 - val_accuracy: 0.2500\n",
            "Epoch 6/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4624 - accuracy: 0.2269 - val_loss: 1.4080 - val_accuracy: 0.2222\n",
            "Epoch 7/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.4390 - accuracy: 0.2361 - val_loss: 1.3955 - val_accuracy: 0.1944\n",
            "Epoch 8/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.4222 - accuracy: 0.2407 - val_loss: 1.3877 - val_accuracy: 0.2083\n",
            "Epoch 9/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.4140 - accuracy: 0.2176 - val_loss: 1.3832 - val_accuracy: 0.2222\n",
            "Epoch 10/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.4060 - accuracy: 0.1944 - val_loss: 1.3810 - val_accuracy: 0.2778\n",
            "Epoch 11/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.4016 - accuracy: 0.2222 - val_loss: 1.3803 - val_accuracy: 0.2361\n",
            "Epoch 12/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3990 - accuracy: 0.2176 - val_loss: 1.3796 - val_accuracy: 0.2361\n",
            "Epoch 13/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3969 - accuracy: 0.2315 - val_loss: 1.3792 - val_accuracy: 0.2778\n",
            "Epoch 14/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3950 - accuracy: 0.2454 - val_loss: 1.3791 - val_accuracy: 0.2639\n",
            "Epoch 15/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3930 - accuracy: 0.2454 - val_loss: 1.3789 - val_accuracy: 0.2639\n",
            "Epoch 16/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3918 - accuracy: 0.2500 - val_loss: 1.3788 - val_accuracy: 0.2639\n",
            "Epoch 17/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3906 - accuracy: 0.2546 - val_loss: 1.3783 - val_accuracy: 0.2639\n",
            "Epoch 18/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3897 - accuracy: 0.2546 - val_loss: 1.3780 - val_accuracy: 0.2639\n",
            "Epoch 19/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3891 - accuracy: 0.2546 - val_loss: 1.3777 - val_accuracy: 0.2639\n",
            "Epoch 20/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3886 - accuracy: 0.2593 - val_loss: 1.3778 - val_accuracy: 0.2639\n",
            "Epoch 21/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3875 - accuracy: 0.2593 - val_loss: 1.3775 - val_accuracy: 0.2639\n",
            "Epoch 22/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3876 - accuracy: 0.2593 - val_loss: 1.3776 - val_accuracy: 0.2639\n",
            "Epoch 23/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3866 - accuracy: 0.2593 - val_loss: 1.3780 - val_accuracy: 0.2639\n",
            "Epoch 24/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3856 - accuracy: 0.2593 - val_loss: 1.3779 - val_accuracy: 0.2639\n",
            "Epoch 25/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3852 - accuracy: 0.2593 - val_loss: 1.3778 - val_accuracy: 0.2639\n",
            "Epoch 26/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3845 - accuracy: 0.2593 - val_loss: 1.3774 - val_accuracy: 0.2639\n",
            "Epoch 27/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3846 - accuracy: 0.2593 - val_loss: 1.3772 - val_accuracy: 0.2639\n",
            "Epoch 28/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3848 - accuracy: 0.2546 - val_loss: 1.3780 - val_accuracy: 0.2917\n",
            "Epoch 29/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3844 - accuracy: 0.2546 - val_loss: 1.3767 - val_accuracy: 0.2778\n",
            "Epoch 30/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3837 - accuracy: 0.2546 - val_loss: 1.3772 - val_accuracy: 0.2917\n",
            "Epoch 31/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3831 - accuracy: 0.2500 - val_loss: 1.3771 - val_accuracy: 0.3056\n",
            "Epoch 32/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3829 - accuracy: 0.2500 - val_loss: 1.3764 - val_accuracy: 0.2917\n",
            "Epoch 33/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3825 - accuracy: 0.2407 - val_loss: 1.3765 - val_accuracy: 0.2917\n",
            "Epoch 34/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3825 - accuracy: 0.2454 - val_loss: 1.3765 - val_accuracy: 0.2917\n",
            "Epoch 35/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3821 - accuracy: 0.2546 - val_loss: 1.3765 - val_accuracy: 0.2917\n",
            "Epoch 36/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3818 - accuracy: 0.2593 - val_loss: 1.3759 - val_accuracy: 0.2500\n",
            "Epoch 37/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3815 - accuracy: 0.2500 - val_loss: 1.3758 - val_accuracy: 0.2639\n",
            "Epoch 38/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3806 - accuracy: 0.2546 - val_loss: 1.3757 - val_accuracy: 0.2917\n",
            "Epoch 39/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3809 - accuracy: 0.2546 - val_loss: 1.3755 - val_accuracy: 0.2917\n",
            "Epoch 40/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3808 - accuracy: 0.2546 - val_loss: 1.3758 - val_accuracy: 0.2917\n",
            "Epoch 41/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3801 - accuracy: 0.2593 - val_loss: 1.3758 - val_accuracy: 0.3056\n",
            "Epoch 42/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3802 - accuracy: 0.2546 - val_loss: 1.3768 - val_accuracy: 0.3056\n",
            "Epoch 43/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3800 - accuracy: 0.2500 - val_loss: 1.3765 - val_accuracy: 0.3056\n",
            "Epoch 44/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3801 - accuracy: 0.2500 - val_loss: 1.3756 - val_accuracy: 0.2917\n",
            "Epoch 45/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3792 - accuracy: 0.2593 - val_loss: 1.3756 - val_accuracy: 0.2639\n",
            "Epoch 46/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3791 - accuracy: 0.2593 - val_loss: 1.3762 - val_accuracy: 0.2500\n",
            "Epoch 47/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3790 - accuracy: 0.2546 - val_loss: 1.3757 - val_accuracy: 0.2639\n",
            "Epoch 48/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3784 - accuracy: 0.2500 - val_loss: 1.3755 - val_accuracy: 0.2639\n",
            "Epoch 49/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3782 - accuracy: 0.2685 - val_loss: 1.3751 - val_accuracy: 0.2917\n",
            "Epoch 50/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3779 - accuracy: 0.2593 - val_loss: 1.3746 - val_accuracy: 0.2917\n",
            "Epoch 51/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3778 - accuracy: 0.2685 - val_loss: 1.3739 - val_accuracy: 0.2917\n",
            "Epoch 52/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3777 - accuracy: 0.2685 - val_loss: 1.3741 - val_accuracy: 0.2778\n",
            "Epoch 53/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3771 - accuracy: 0.2639 - val_loss: 1.3744 - val_accuracy: 0.2917\n",
            "Epoch 54/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3773 - accuracy: 0.2546 - val_loss: 1.3752 - val_accuracy: 0.2917\n",
            "Epoch 55/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3767 - accuracy: 0.2593 - val_loss: 1.3750 - val_accuracy: 0.2917\n",
            "Epoch 56/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3763 - accuracy: 0.2685 - val_loss: 1.3744 - val_accuracy: 0.2917\n",
            "Epoch 57/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3761 - accuracy: 0.2639 - val_loss: 1.3744 - val_accuracy: 0.2639\n",
            "Epoch 58/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3760 - accuracy: 0.2685 - val_loss: 1.3742 - val_accuracy: 0.2639\n",
            "Epoch 59/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3763 - accuracy: 0.2500 - val_loss: 1.3741 - val_accuracy: 0.2361\n",
            "Epoch 60/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3762 - accuracy: 0.2500 - val_loss: 1.3737 - val_accuracy: 0.2500\n",
            "Epoch 61/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3760 - accuracy: 0.2546 - val_loss: 1.3736 - val_accuracy: 0.2639\n",
            "Epoch 62/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3756 - accuracy: 0.2500 - val_loss: 1.3736 - val_accuracy: 0.2500\n",
            "Epoch 63/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3761 - accuracy: 0.2546 - val_loss: 1.3737 - val_accuracy: 0.2500\n",
            "Epoch 64/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3754 - accuracy: 0.2500 - val_loss: 1.3736 - val_accuracy: 0.2639\n",
            "Epoch 65/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3745 - accuracy: 0.2546 - val_loss: 1.3739 - val_accuracy: 0.2361\n",
            "Epoch 66/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3744 - accuracy: 0.2546 - val_loss: 1.3739 - val_accuracy: 0.2500\n",
            "Epoch 67/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3744 - accuracy: 0.2500 - val_loss: 1.3746 - val_accuracy: 0.2639\n",
            "Epoch 68/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3737 - accuracy: 0.2500 - val_loss: 1.3742 - val_accuracy: 0.2500\n",
            "Epoch 69/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3737 - accuracy: 0.2593 - val_loss: 1.3749 - val_accuracy: 0.2500\n",
            "Epoch 70/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3729 - accuracy: 0.2500 - val_loss: 1.3748 - val_accuracy: 0.2639\n",
            "Epoch 71/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3729 - accuracy: 0.2500 - val_loss: 1.3741 - val_accuracy: 0.2361\n",
            "Epoch 72/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3723 - accuracy: 0.2685 - val_loss: 1.3744 - val_accuracy: 0.2222\n",
            "Epoch 73/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3719 - accuracy: 0.2685 - val_loss: 1.3744 - val_accuracy: 0.2361\n",
            "Epoch 74/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3718 - accuracy: 0.2639 - val_loss: 1.3751 - val_accuracy: 0.2500\n",
            "Epoch 75/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3716 - accuracy: 0.2685 - val_loss: 1.3755 - val_accuracy: 0.2500\n",
            "Epoch 76/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3711 - accuracy: 0.2639 - val_loss: 1.3752 - val_accuracy: 0.2500\n",
            "Epoch 77/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3707 - accuracy: 0.2639 - val_loss: 1.3755 - val_accuracy: 0.2500\n",
            "Epoch 78/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3706 - accuracy: 0.2778 - val_loss: 1.3753 - val_accuracy: 0.2500\n",
            "Epoch 79/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3703 - accuracy: 0.2639 - val_loss: 1.3761 - val_accuracy: 0.2500\n",
            "Epoch 80/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3698 - accuracy: 0.2685 - val_loss: 1.3758 - val_accuracy: 0.2500\n",
            "Epoch 81/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3693 - accuracy: 0.2639 - val_loss: 1.3764 - val_accuracy: 0.2361\n",
            "Epoch 82/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3695 - accuracy: 0.2685 - val_loss: 1.3776 - val_accuracy: 0.2778\n",
            "Epoch 83/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3693 - accuracy: 0.2870 - val_loss: 1.3779 - val_accuracy: 0.2778\n",
            "Epoch 84/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3694 - accuracy: 0.2963 - val_loss: 1.3773 - val_accuracy: 0.2500\n",
            "Epoch 85/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3681 - accuracy: 0.3056 - val_loss: 1.3772 - val_accuracy: 0.2778\n",
            "Epoch 86/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3684 - accuracy: 0.2963 - val_loss: 1.3769 - val_accuracy: 0.2639\n",
            "Epoch 87/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3673 - accuracy: 0.3009 - val_loss: 1.3769 - val_accuracy: 0.2639\n",
            "Epoch 88/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3674 - accuracy: 0.3056 - val_loss: 1.3776 - val_accuracy: 0.2500\n",
            "Epoch 89/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3662 - accuracy: 0.3148 - val_loss: 1.3771 - val_accuracy: 0.2500\n",
            "Epoch 90/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3660 - accuracy: 0.3056 - val_loss: 1.3772 - val_accuracy: 0.2500\n",
            "Epoch 91/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3654 - accuracy: 0.2917 - val_loss: 1.3779 - val_accuracy: 0.2222\n",
            "Epoch 92/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3644 - accuracy: 0.2870 - val_loss: 1.3783 - val_accuracy: 0.2639\n",
            "Epoch 93/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3642 - accuracy: 0.2917 - val_loss: 1.3786 - val_accuracy: 0.2500\n",
            "Epoch 94/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3641 - accuracy: 0.2917 - val_loss: 1.3786 - val_accuracy: 0.2500\n",
            "Epoch 95/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3636 - accuracy: 0.2870 - val_loss: 1.3794 - val_accuracy: 0.2361\n",
            "Epoch 96/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3636 - accuracy: 0.2685 - val_loss: 1.3800 - val_accuracy: 0.2222\n",
            "Epoch 97/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3626 - accuracy: 0.2731 - val_loss: 1.3799 - val_accuracy: 0.2222\n",
            "Epoch 98/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3618 - accuracy: 0.2731 - val_loss: 1.3793 - val_accuracy: 0.2639\n",
            "Epoch 99/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3624 - accuracy: 0.2824 - val_loss: 1.3791 - val_accuracy: 0.2361\n",
            "Epoch 100/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3618 - accuracy: 0.2870 - val_loss: 1.3793 - val_accuracy: 0.2361\n",
            "Epoch 101/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3608 - accuracy: 0.2917 - val_loss: 1.3786 - val_accuracy: 0.2083\n",
            "Epoch 102/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3613 - accuracy: 0.2963 - val_loss: 1.3789 - val_accuracy: 0.2500\n",
            "Epoch 103/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3604 - accuracy: 0.2824 - val_loss: 1.3797 - val_accuracy: 0.2222\n",
            "Epoch 104/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3600 - accuracy: 0.2778 - val_loss: 1.3798 - val_accuracy: 0.2222\n",
            "Epoch 105/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3595 - accuracy: 0.2870 - val_loss: 1.3797 - val_accuracy: 0.2222\n",
            "Epoch 106/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3592 - accuracy: 0.2917 - val_loss: 1.3796 - val_accuracy: 0.2639\n",
            "Epoch 107/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.3587 - accuracy: 0.2870 - val_loss: 1.3795 - val_accuracy: 0.2639\n",
            "Epoch 108/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3586 - accuracy: 0.2870 - val_loss: 1.3799 - val_accuracy: 0.2361\n",
            "Epoch 109/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3584 - accuracy: 0.2963 - val_loss: 1.3793 - val_accuracy: 0.2778\n",
            "Epoch 110/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3577 - accuracy: 0.2824 - val_loss: 1.3796 - val_accuracy: 0.2500\n",
            "Epoch 111/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3574 - accuracy: 0.2778 - val_loss: 1.3798 - val_accuracy: 0.2917\n",
            "Epoch 112/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3572 - accuracy: 0.2963 - val_loss: 1.3798 - val_accuracy: 0.2500\n",
            "Epoch 113/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3564 - accuracy: 0.2917 - val_loss: 1.3801 - val_accuracy: 0.2222\n",
            "Epoch 114/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3565 - accuracy: 0.2917 - val_loss: 1.3813 - val_accuracy: 0.2083\n",
            "Epoch 115/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3570 - accuracy: 0.2917 - val_loss: 1.3813 - val_accuracy: 0.2500\n",
            "Epoch 116/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3560 - accuracy: 0.2963 - val_loss: 1.3819 - val_accuracy: 0.2361\n",
            "Epoch 117/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3561 - accuracy: 0.2870 - val_loss: 1.3831 - val_accuracy: 0.2083\n",
            "Epoch 118/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3556 - accuracy: 0.2870 - val_loss: 1.3830 - val_accuracy: 0.2222\n",
            "Epoch 119/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3556 - accuracy: 0.2778 - val_loss: 1.3819 - val_accuracy: 0.2639\n",
            "Epoch 120/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3548 - accuracy: 0.2731 - val_loss: 1.3817 - val_accuracy: 0.2500\n",
            "Epoch 121/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3548 - accuracy: 0.2917 - val_loss: 1.3814 - val_accuracy: 0.2778\n",
            "Epoch 122/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3546 - accuracy: 0.2963 - val_loss: 1.3812 - val_accuracy: 0.2778\n",
            "Epoch 123/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3547 - accuracy: 0.2778 - val_loss: 1.3813 - val_accuracy: 0.2917\n",
            "Epoch 124/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3544 - accuracy: 0.2824 - val_loss: 1.3812 - val_accuracy: 0.2778\n",
            "Epoch 125/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3541 - accuracy: 0.2824 - val_loss: 1.3810 - val_accuracy: 0.2917\n",
            "Epoch 126/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3540 - accuracy: 0.2870 - val_loss: 1.3814 - val_accuracy: 0.2917\n",
            "Epoch 127/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3536 - accuracy: 0.2917 - val_loss: 1.3814 - val_accuracy: 0.3056\n",
            "Epoch 128/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3534 - accuracy: 0.2917 - val_loss: 1.3813 - val_accuracy: 0.2917\n",
            "Epoch 129/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3541 - accuracy: 0.2963 - val_loss: 1.3809 - val_accuracy: 0.2917\n",
            "Epoch 130/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3531 - accuracy: 0.3102 - val_loss: 1.3813 - val_accuracy: 0.3194\n",
            "Epoch 131/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3533 - accuracy: 0.2824 - val_loss: 1.3822 - val_accuracy: 0.2639\n",
            "Epoch 132/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3532 - accuracy: 0.2870 - val_loss: 1.3821 - val_accuracy: 0.2917\n",
            "Epoch 133/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3524 - accuracy: 0.2870 - val_loss: 1.3832 - val_accuracy: 0.2639\n",
            "Epoch 134/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3533 - accuracy: 0.2870 - val_loss: 1.3840 - val_accuracy: 0.2361\n",
            "Epoch 135/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3515 - accuracy: 0.2963 - val_loss: 1.3832 - val_accuracy: 0.3056\n",
            "Epoch 136/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3525 - accuracy: 0.2963 - val_loss: 1.3833 - val_accuracy: 0.2917\n",
            "Epoch 137/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3519 - accuracy: 0.2917 - val_loss: 1.3841 - val_accuracy: 0.2778\n",
            "Epoch 138/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3515 - accuracy: 0.2963 - val_loss: 1.3843 - val_accuracy: 0.2917\n",
            "Epoch 139/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3512 - accuracy: 0.2824 - val_loss: 1.3848 - val_accuracy: 0.2917\n",
            "Epoch 140/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3516 - accuracy: 0.2870 - val_loss: 1.3858 - val_accuracy: 0.2778\n",
            "Epoch 141/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3510 - accuracy: 0.2917 - val_loss: 1.3867 - val_accuracy: 0.2639\n",
            "Epoch 142/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3516 - accuracy: 0.3009 - val_loss: 1.3858 - val_accuracy: 0.3056\n",
            "Epoch 143/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3500 - accuracy: 0.2917 - val_loss: 1.3853 - val_accuracy: 0.2917\n",
            "Epoch 144/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3497 - accuracy: 0.2963 - val_loss: 1.3852 - val_accuracy: 0.2917\n",
            "Epoch 145/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3495 - accuracy: 0.2963 - val_loss: 1.3851 - val_accuracy: 0.2778\n",
            "Epoch 146/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3487 - accuracy: 0.3056 - val_loss: 1.3861 - val_accuracy: 0.2778\n",
            "Epoch 147/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3489 - accuracy: 0.3009 - val_loss: 1.3862 - val_accuracy: 0.2639\n",
            "Epoch 148/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3485 - accuracy: 0.2963 - val_loss: 1.3845 - val_accuracy: 0.2778\n",
            "Epoch 149/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3473 - accuracy: 0.3148 - val_loss: 1.3842 - val_accuracy: 0.2639\n",
            "Epoch 150/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3471 - accuracy: 0.3148 - val_loss: 1.3842 - val_accuracy: 0.2639\n",
            "Epoch 151/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3470 - accuracy: 0.3056 - val_loss: 1.3852 - val_accuracy: 0.2639\n",
            "Epoch 152/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3467 - accuracy: 0.3102 - val_loss: 1.3848 - val_accuracy: 0.2639\n",
            "Epoch 153/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3461 - accuracy: 0.3148 - val_loss: 1.3857 - val_accuracy: 0.2639\n",
            "Epoch 154/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3461 - accuracy: 0.3287 - val_loss: 1.3879 - val_accuracy: 0.2639\n",
            "Epoch 155/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3453 - accuracy: 0.3148 - val_loss: 1.3891 - val_accuracy: 0.2778\n",
            "Epoch 156/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3459 - accuracy: 0.3102 - val_loss: 1.3899 - val_accuracy: 0.2500\n",
            "Epoch 157/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3453 - accuracy: 0.2963 - val_loss: 1.3897 - val_accuracy: 0.2500\n",
            "Epoch 158/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3447 - accuracy: 0.3009 - val_loss: 1.3905 - val_accuracy: 0.2639\n",
            "Epoch 159/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3434 - accuracy: 0.3102 - val_loss: 1.3903 - val_accuracy: 0.2639\n",
            "Epoch 160/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3431 - accuracy: 0.3241 - val_loss: 1.3920 - val_accuracy: 0.2778\n",
            "Epoch 161/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3427 - accuracy: 0.3148 - val_loss: 1.3912 - val_accuracy: 0.2778\n",
            "Epoch 162/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3425 - accuracy: 0.3287 - val_loss: 1.3909 - val_accuracy: 0.2778\n",
            "Epoch 163/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3420 - accuracy: 0.3102 - val_loss: 1.3909 - val_accuracy: 0.2778\n",
            "Epoch 164/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3420 - accuracy: 0.3194 - val_loss: 1.3928 - val_accuracy: 0.2500\n",
            "Epoch 165/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3413 - accuracy: 0.3333 - val_loss: 1.3932 - val_accuracy: 0.2222\n",
            "Epoch 166/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3412 - accuracy: 0.3194 - val_loss: 1.3937 - val_accuracy: 0.2083\n",
            "Epoch 167/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3403 - accuracy: 0.3194 - val_loss: 1.3949 - val_accuracy: 0.2500\n",
            "Epoch 168/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3398 - accuracy: 0.3333 - val_loss: 1.3941 - val_accuracy: 0.2500\n",
            "Epoch 169/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3395 - accuracy: 0.3333 - val_loss: 1.3947 - val_accuracy: 0.2500\n",
            "Epoch 170/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3390 - accuracy: 0.3287 - val_loss: 1.3959 - val_accuracy: 0.2361\n",
            "Epoch 171/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3385 - accuracy: 0.3148 - val_loss: 1.3979 - val_accuracy: 0.2778\n",
            "Epoch 172/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3387 - accuracy: 0.3102 - val_loss: 1.3958 - val_accuracy: 0.2222\n",
            "Epoch 173/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3382 - accuracy: 0.3333 - val_loss: 1.3966 - val_accuracy: 0.2222\n",
            "Epoch 174/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3368 - accuracy: 0.3241 - val_loss: 1.4001 - val_accuracy: 0.2778\n",
            "Epoch 175/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3356 - accuracy: 0.3056 - val_loss: 1.4040 - val_accuracy: 0.2083\n",
            "Epoch 176/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3353 - accuracy: 0.3194 - val_loss: 1.4023 - val_accuracy: 0.2500\n",
            "Epoch 177/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3338 - accuracy: 0.3241 - val_loss: 1.4019 - val_accuracy: 0.2639\n",
            "Epoch 178/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3330 - accuracy: 0.3287 - val_loss: 1.4022 - val_accuracy: 0.2500\n",
            "Epoch 179/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3319 - accuracy: 0.3194 - val_loss: 1.4030 - val_accuracy: 0.2500\n",
            "Epoch 180/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3318 - accuracy: 0.3148 - val_loss: 1.4020 - val_accuracy: 0.2500\n",
            "Epoch 181/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3303 - accuracy: 0.2963 - val_loss: 1.4038 - val_accuracy: 0.2361\n",
            "Epoch 182/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3290 - accuracy: 0.3194 - val_loss: 1.4052 - val_accuracy: 0.1944\n",
            "Epoch 183/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3283 - accuracy: 0.3102 - val_loss: 1.4060 - val_accuracy: 0.2222\n",
            "Epoch 184/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3285 - accuracy: 0.3009 - val_loss: 1.4073 - val_accuracy: 0.1944\n",
            "Epoch 185/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3265 - accuracy: 0.3102 - val_loss: 1.4086 - val_accuracy: 0.2083\n",
            "Epoch 186/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3245 - accuracy: 0.3241 - val_loss: 1.4108 - val_accuracy: 0.1806\n",
            "Epoch 187/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3235 - accuracy: 0.3426 - val_loss: 1.4112 - val_accuracy: 0.1667\n",
            "Epoch 188/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3219 - accuracy: 0.3472 - val_loss: 1.4117 - val_accuracy: 0.1528\n",
            "Epoch 189/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3212 - accuracy: 0.3380 - val_loss: 1.4103 - val_accuracy: 0.1806\n",
            "Epoch 190/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3202 - accuracy: 0.3287 - val_loss: 1.4141 - val_accuracy: 0.1806\n",
            "Epoch 191/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3204 - accuracy: 0.3333 - val_loss: 1.4146 - val_accuracy: 0.1806\n",
            "Epoch 192/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3195 - accuracy: 0.3194 - val_loss: 1.4129 - val_accuracy: 0.1806\n",
            "Epoch 193/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3192 - accuracy: 0.3056 - val_loss: 1.4158 - val_accuracy: 0.1944\n",
            "Epoch 194/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3178 - accuracy: 0.3472 - val_loss: 1.4135 - val_accuracy: 0.1944\n",
            "Epoch 195/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3170 - accuracy: 0.3333 - val_loss: 1.4158 - val_accuracy: 0.1944\n",
            "Epoch 196/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3171 - accuracy: 0.3472 - val_loss: 1.4155 - val_accuracy: 0.1944\n",
            "Epoch 197/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3164 - accuracy: 0.3426 - val_loss: 1.4169 - val_accuracy: 0.1806\n",
            "Epoch 198/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3157 - accuracy: 0.3519 - val_loss: 1.4187 - val_accuracy: 0.1944\n",
            "Epoch 199/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3148 - accuracy: 0.3519 - val_loss: 1.4189 - val_accuracy: 0.1806\n",
            "Epoch 200/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3143 - accuracy: 0.3287 - val_loss: 1.4198 - val_accuracy: 0.1806\n",
            "Epoch 201/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.3138 - accuracy: 0.3472 - val_loss: 1.4193 - val_accuracy: 0.2083\n",
            "Epoch 202/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3142 - accuracy: 0.3333 - val_loss: 1.4186 - val_accuracy: 0.1667\n",
            "Epoch 203/1000\n",
            " 1/11 [=>............................] - ETA: 0s - loss: 1.2195 - accuracy: 0.5714"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-5ad36129b415>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit the keras model on the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can visualize the results with a confusion matrix.\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_confusion_matrix(y_classified, y_true):\n",
        "  # Compute confusion matrix\n",
        "  c_mat = np.zeros((num_classes,num_classes))\n",
        "  for i in range(len(y_true)):\n",
        "    c_mat[y_classified[i], y_true[i] ] += 1\n",
        "\n",
        "  group_counts = [\"{0:0.0f}\".format(value) for value in c_mat.flatten()]\n",
        "  group_percentages = [\"{0:.2%}\".format(value) for value in c_mat.flatten()/np.sum(c_mat)]\n",
        "  labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_counts, group_percentages)]\n",
        "  labels = np.asarray(labels).reshape(c_mat.shape[0], c_mat.shape[1])\n",
        "\n",
        "  plt.figure(figsize=(12,10))\n",
        "  sn.heatmap(c_mat, annot=labels, fmt='', cmap='rocket_r')\n",
        "  plt.title(\"Confusion Matrix\")\n",
        "  plt.ylabel('Output Class')\n",
        "  plt.xlabel('Target Class')"
      ],
      "metadata": {
        "id": "O91LTNFgDXOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the trained model using keras built-in function\n",
        "score = net.evaluate(X_test, y_test, verbose=1)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1]) \n",
        "\n",
        "y_classified = np.argmax(net.predict(X_test), axis=1)\n",
        "y_true = np.argmax(y_test.to_numpy(), axis=1)\n",
        "# plot confusion matrix\n",
        "plot_confusion_matrix(y_classified, y_true)"
      ],
      "metadata": {
        "id": "01q6CPi0DYZd",
        "outputId": "5e5ba59a-33d3-4dc0-d27a-448ec120db6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 4ms/step - loss: 1.5496 - accuracy: 0.1806\n",
            "Test loss: 1.5495566129684448\n",
            "Test accuracy: 0.1805555522441864\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x720 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "bcvu_gzYDaex"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}